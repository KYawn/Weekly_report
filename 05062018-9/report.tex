%!TEX program = xelatex
%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[a4paper]{article}
\usepackage[UTF8, heading = false, scheme = plain]{ctex}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{geometry}
\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}
\usepackage{subfig}
\usepackage{caption}
\captionsetup{font={scriptsize}}

\renewcommand\figurename{图}

\makeatletter
\let\@afterindentfalse\@afterindenttrue
\@afterindenttrue
\makeatother
\setlength{\parindent}{2em}  

\linespread{1.4}
\setlength{\parskip}{0.5\baselineskip}

\title{学习汇报\\第九周}
\author{熊凯亚}
\date{\today}

\begin{document}
\maketitle

因为第七周主要看的是Google在ICLR'17上发表的文章\cite{Papernot2016SemisupervisedKT}用半监督知识迁移解决深度学习中训练数据隐私问题，为了解决这个问题这篇文章提出了PATE(Private Aggregation of Teacher Ensembles)教师全体的隐私聚合的概念。

本周继续看了这方面的论文可扩展的PATE，看了Google团队在ICLR'18上发表的文章\cite{Papernot2018ScalablePL}。因为PATE仅仅在简单的分类任务（MNIST）上进行了评估，并没有应用于更大规模的学习任务和实际数据集。

这篇文章讲的是怎样将PATE扩展成适用于具有大量输出类的学习任务及不均衡的训练数据中。本文引入了新的噪声聚合机制，这种机制具有更多的选择性、只添加了很少的噪音，并且证明了这种机制有更加强的差分隐私保护。本文发现：使用更集中的噪音可以增加教师之间的共识并且，如果教师之间没有共识，那就不需要回应学生模型的查询。这样的话就可以同时具备高实用性和强隐私性($\epsilon<1.0$)。
\section{PATE回顾}
\subsection*{教师模型}

首先，将待训练的敏感数据分为互斥的$N$份不同数据集，然后由这些数据集分别独立训练不同的模型，得到$N$个教师模型。当部署训练好的教师模型时，需要记录每一个教师模型对于查询的预测结果，选取票数最高的那个，并将预测结果聚合起来。如果大部分教师模型都同意某一个预测结果，那就意味着它不依赖于具体的分散数据集，所以隐私成本很小。但是，如果有两类预测结果有相近的票数，那么这种不一致，或许会泄露隐私信息。

因此，在中间统计最高票数之后引入拉普拉斯噪声，将票数的统计情况打乱，从而保护了隐私。教师模型的训练及聚集过程如图\ref*{fig:overview}左侧部分所示。

\begin{figure*}[!ht]
\includegraphics[width = \linewidth]{fig/approach-overview.pdf}
\caption{教师模型的聚集过程（左侧部分）和学生模型（右侧部分）}
\label{fig:overview}
\end{figure*}

\subsection*{学生模型}

聚合教师模型（Aggregated Teacher）可以看作是一个差分隐私API，用户提交输入值，他就会返回相应的标签，同时又能保护隐私。但是，如果能训练一个机器学习模型，部署到用户设备上直接运行模型得出预测结果，这样的话肯定会更好。因此，又训练了一个额外模型：学生模型。学生模型可以获得未标记的公共数据集。为了训练学生模型，需要聚合教师模型以隐私保护的方式，来给公共数据进行标注，传递知识。用于部署在设备上的，就是训练好的学生模型。
\subsection*{学生模型的必要性}

实际上聚合教师模型破坏了之前所说的威胁模型，每次在查询聚合教师模型时，都会增加隐私成本，因为它每次给出的输出结果都会或多或少地透露出一些隐私信息。然而，当学生模型训练好之后，只能对聚合教师模型进行固定数量的查询，那么隐私成本就会被固定下来了。

另外，还需要防范攻击者探取模型底层函数库。教师模型是由隐私数据训练的，学生模型是由公共数据（非隐私数据）训练的，带有隐私保护的标注。所以最坏的情况是，攻击者通过查验学生模型的底层函数库而获得其训练数据，即使这样攻击者也只能得到带有隐私保护的标注信息，除此以外攻击者得不到再多的隐私信息了。
\subsection*{PATE-G}

PATE-G是PATE的一种生成式变种，它使用对抗生成网络GAN来减少训练学生模型时所需的标签数目。PATE-G的设计初衷很简单：我们希望产生学生模型训练时需要用的标签数目，数目越小，则隐私成本越小。

生成对抗网络（GANs）的一般架构是分为生成器和判别器。我们将原本二元分类的判别器（只判别数据是真实的 or 生成的）扩展至一个多类别的分类器，用来区分：已标注的真实样本，未标注真实样本，以及生成样本\cite{Salimans2016ImprovedTF}。


\section{可扩展的PATE}
PATE中教师模型在互不相交的训练数据集上分别训练自己独立的模型，这些训练数据有可能是敏感数据。PATE使用教师模型聚集的共识答案以黑盒的方式来监督训练学生模型。然而原始的PATE\cite{Papernot2016SemisupervisedKT}只在MNIST上做了实验，并没有在大量实验数据的情况下做实验验证其可行性。
本文主要讲了
\begin{itemize}
\item 在原有PATE方案基础上对大量数据集进行验证。
\item 改善了原有PATE方案中教师模型的聚集机制。
\item 本方案只需添加更少的噪声即可保护隐私。
\end{itemize}

\subsection{聚集机制}

在PATE中，聚集机制主要是将拉普拉斯噪声添加到教师模型的投票结果中并输出最高票数的类，这样就可以在保护隐私的同时对学生模型进行训练。本文使用高斯噪声替代了原始PATE中添加的拉普拉斯噪声，这样对于学生模型的每次查询，只需要添加很少的噪音即可达到相同的隐私消耗。这种聚集机制是选择性的：这种机制可以分析教师的投票来判断哪些学生的查询值得回答。这样就同时考虑了每个查询的隐私成本和为了提高学生模型可用性的成本。实际上这两者并不矛盾：当教师投票同意时隐私成本最小，因此对于学生来说，当教师同意时标签是正确的可能性会更大。另外文中还提出了一种教师和学生的交互机制，这种机制不仅考虑到了教师的投票，还考虑到了学生会对他的这次查询的结果的预测。在此基础上，那些值得教师回答的问题就是：教师之间对类达成了一直，但是学生对这个类的预测结果不太确定。这样一来，并不需要为那些学生已经同意教师的共识的查询花费隐私预算，并且那些学生不太确定预测结果的查询也只需要很少的额隐私成本。

为了减少噪声，新的PATE聚合机制对高斯噪声进行采样，因为该分布的尾部比原始PATE中使用的拉普拉斯噪声的尾部降低的更快。当PATE被应用到具有大量输出类别的学习任务时，这就会极大地增加教师在共识答案中的投票结果的噪音聚集机会。

\subsection{Confident-GNMAX 聚集机制}
本文提出了一种对于高斯噪音(GNMAX)聚集器的改进，使用这种聚集器可以筛选出哪些教师对于预测结果没有达成共识。有了这种筛选机制，教师们就可以不用回答那些耗费昂贵的查询。

Confident Aggregator中，算法会检查教师的投票是否超过阈值$T$，以确保教师们能达成高度一致。为了在这个步骤中添加隐私保护，对教师投票的结果添加方差为$\sigma_1^2$的高斯噪声，然后对其进行比较。对于通过噪声阈值检查的查询，聚集器使用常用的具有更小方差$\sigma_2^2$的GNMAX机制；对于那些没有通过噪声阈值检查的查询，聚集器只会返回$\perp$，并且在训练学生模型时不使用这个结果。

这个聚集器的隐私成本是，每个查询的阈值检查费用，而GNMAX只有通过检查的查询的费用。相比之下，Confident Aggregator花费较小的隐私成本来检查阈值，并且因为它只回答较少的昂贵查询，所以只花费了较低的隐私成本。

\subsection{Interactive-GNMAX 聚集机制}

虽然Confident Aggregator排除了代价昂贵的查询，但它忽略了学生可能会收到对学习贡献不大的标签，从而影响其实用性。通过将学生当前的预测结合到公开训练数据集中，本文设计了一个交互式在这个聚集器中，那些学生已经有意识地预测且与教师相同的标签的查询都会被抛弃。

给定一组查询，交互式聚集器通过比较学生预测和每个类别的教师投票选择正确的答案。对于未通过此项检查的查询，如果学生足够信任并且无需再次查看教师投票，则该机制会强化预测的学生标签。这种有限的监督形式只需要很小的隐私成本。此外，如果学生不同意教师的共识，则检查顺序可确保学生对其查询的预测错误地加以确认并不会被意外强化。


\subsection{总结}
为什么要向本文中提出的两种聚集机制中加入噪声呢？因为聚集输出的标签的隐私性和准确性之间存在着一种协同作用（联合效应）：花费很少隐私成本的标签也很可能是正确的。因此，当教师之间的共识太低而不能在隐私中以小成本提供预测结果时，可以通过不输出标签来为训练学生模型提供更多准确性提升。这种结果在实验中得到了进一步的证实，实验观察到对于固定数量的标签，如对学生进行私有或非私有标签的训练，前者几乎总是比后者表现更好。

\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
